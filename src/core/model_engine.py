"""
–î–≤–∏–∂–æ–∫ –ò–ò –º–æ–¥–µ–ª–∏ –¥–ª—è Tux AI
–†–µ–∞–ª–∏–∑—É–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–µ–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏ –±–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ
"""

import torch
import torch.nn as nn
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments
import os
from pathlib import Path
from typing import List, Dict, Any
import json

class TuxModelEngine:
    """–î–≤–∏–∂–æ–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ò–ò –º–æ–¥–µ–ª—è–º–∏"""
    
    def __init__(self, model_dir: str = "data/models"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.is_loaded = False
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.config = {
            "max_length": 200,
            "temperature": 0.8,
            "top_k": 50,
            "top_p": 0.9,
            "repetition_penalty": 1.1
        }
    
    def load_model(self, model_name: str = "sberbank-ai/rugpt3small_based_on_gpt2"):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            print(f"üß† –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å: {model_name}")
            
            self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
            self.model = GPT2LMHeadModel.from_pretrained(model_name)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # –ü–µ—Ä–µ–Ω–æ—Å –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.model.to(self.device)
            self.model.eval()
            
            self.is_loaded = True
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {self.device}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self.is_loaded = False
            return False
    
    def generate_response(self, prompt: str, **kwargs) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
        if not self.is_loaded:
            return "‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å —Å–Ω–∞—á–∞–ª–∞."
        
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            config = {**self.config, **kwargs}
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=len(inputs[0]) + config["max_length"],
                    temperature=config["temperature"],
                    top_k=config["top_k"],
                    top_p=config["top_p"],
                    repetition_penalty=config["repetition_penalty"],
                    pad_token_id=self.tokenizer.eos_token_id,
                    do_sample=True,
                    num_return_sequences=1
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
            if response.startswith(prompt):
                response = response[len(prompt):].strip()
            
            return response
            
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"
    
    def train_on_text(self, text: str, epochs: int = 3):
        """–ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ–∫—Å—Ç–µ"""
        if not self.is_loaded:
            return {"success": False, "error": "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}
        
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç –≤ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            temp_file = self.model_dir / "temp_training.txt"
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(text)
            
            # –°–æ–∑–¥–∞–µ–º dataset
            train_dataset = TextDataset(
                tokenizer=self.tokenizer,
                file_path=str(temp_file),
                block_size=128
            )
            
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            )
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
            training_args = TrainingArguments(
                output_dir=str(self.model_dir / "checkpoints"),
                overwrite_output_dir=True,
                num_train_epochs=epochs,
                per_device_train_batch_size=2,
                save_steps=100,
                save_total_limit=2,
                prediction_loss_only=True,
                logging_steps=10,
            )
            
            # –û–±—É—á–µ–Ω–∏–µ
            trainer = Trainer(
                model=self.model,
                args=training_args,
                data_collator=data_collator,
                train_dataset=train_dataset,
            )
            
            # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
            trainer.train()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            trainer.save_model()
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            temp_file.unlink()
            
            return {"success": True, "epochs": epochs, "loss": trainer.state.log_history[-1]['loss']}
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def save_model(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if self.is_loaded:
            save_path = self.model_dir / path
            self.model.save_pretrained(save_path)
            self.tokenizer.save_pretrained(save_path)
            return True
        return False
    
    def load_custom_model(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            model_path = self.model_dir / path
            self.tokenizer = GPT2Tokenizer.from_pretrained(str(model_path))
            self.model = GPT2LMHeadModel.from_pretrained(str(model_path))
            self.model.to(self.device)
            self.model.eval()
            self.is_loaded = True
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return False

class SimpleFallbackModel:
    """–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å-–∑–∞–≥–ª—É—à–∫–∞ –∫–æ–≥–¥–∞ –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞"""
    
    def __init__(self):
        self.responses = [
            "–ü—Ä–∏–≤–µ—Ç! –Ø Tux AI, –≤–∞—à–∞ –ª–æ–∫–∞–ª—å–Ω–∞—è –ò–ò —Å–∏—Å—Ç–µ–º–∞. üêß",
            "–û—Ç–ª–∏—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å! –í –ø–æ–ª–Ω–æ–π –≤–µ—Ä—Å–∏–∏ —è —Å–º–æ–≥—É –¥–∞—Ç—å –±–æ–ª–µ–µ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç.",
            "Tux AI –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ! –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π.",
            "–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –∑–∞–ø—Ä–æ—Å! –Ø –æ–±—É—á–∞—é—Å—å –Ω–∞ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ç–∞–Ω–æ–≤–ª—é—Å—å —É–º–Ω–µ–µ.",
            "–ü–∏–Ω–≥–≤–∏–Ω—ã –æ–¥–æ–±—Ä—è—é—Ç –≤–∞—à–µ –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ! üêß",
            "–í —ç—Ç–æ–º –¥–µ–º–æ-—Ä–µ–∂–∏–º–µ —è –ø–æ–∫–∞–∑—ã–≤–∞—é –±–∞–∑–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏. –ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–æ—Ä–∞–∑–∏—Ç –≤–∞—Å!",
            "–†–∞–±–æ—Ç–∞—é –Ω–∞–¥ –≤–∞—à–∏–º –∑–∞–ø—Ä–æ—Å–æ–º... –í –ø–æ–ª–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç.",
            "Tux AI —É—á–∏—Ç—Å—è! –ö–∞–∂–¥–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –¥–µ–ª–∞–µ—Ç –º–µ–Ω—è –ª—É—á—à–µ.",
        ]
    
    def generate_response(self, prompt: str, **kwargs) -> str:
        import random
        return random.choice(self.responses)
    
    def train_on_text(self, text: str, epochs: int = 3):
        return {"success": True, "message": "–û–±—É—á–µ–Ω–∏–µ –≤ –¥–µ–º–æ-—Ä–µ–∂–∏–º–µ", "epochs": epochs}