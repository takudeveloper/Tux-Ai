"""
–°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å Tux AI —Å 1000 –Ω–µ–π—Ä–æ–Ω–æ–≤
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from PIL import Image
import cv2
from pathlib import Path
from typing import List, Dict, Any, Optional
import json

class TuxNeuralNetwork(nn.Module):
    """
    –°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å Tux AI —Å —Ç–æ—á–Ω–æ 1000 –Ω–µ–π—Ä–æ–Ω–æ–≤
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: 784 (–≤—Ö–æ–¥) -> 512 -> 256 -> 128 -> 64 -> 32 -> 16 -> 8 (–≤—ã—Ö–æ–¥)
    –ò—Ç–æ–≥–æ: 784 + 512 + 256 + 128 + 64 + 32 + 16 + 8 = 1800 —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π, –Ω–æ 1000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤
    """
    
    def __init__(self, input_size=784, output_size=10):
        super(TuxNeuralNetwork, self).__init__()
        
        # –¢–æ—á–Ω–æ 1000 –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö
        self.fc1 = nn.Linear(input_size, 400)    # 400 –Ω–µ–π—Ä–æ–Ω–æ–≤
        self.fc2 = nn.Linear(400, 300)           # 300 –Ω–µ–π—Ä–æ–Ω–æ–≤  
        self.fc3 = nn.Linear(300, 200)           # 200 –Ω–µ–π—Ä–æ–Ω–æ–≤
        self.fc4 = nn.Linear(200, 100)           # 100 –Ω–µ–π—Ä–æ–Ω–æ–≤
        self.fc5 = nn.Linear(100, output_size)   # –≤—ã—Ö–æ–¥–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω—ã
        
        # –ò—Ç–æ–≥–æ: 400 + 300 + 200 + 100 = 1000 —Å–∫—Ä—ã—Ç—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤
        
        self.dropout = nn.Dropout(0.2)
        self.activation = nn.ReLU()
        
        # –°—á–µ—Ç—á–∏–∫ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.neuron_activations = np.zeros(1000)
        self.total_activations = 0
        
    def forward(self, x):
        # Flatten –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        x = x.view(x.size(0), -1)
        
        # –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ —Å–µ—Ç—å
        x = self.activation(self.fc1(x))  # 400 –Ω–µ–π—Ä–æ–Ω–æ–≤
        x = self.dropout(x)
        
        x = self.activation(self.fc2(x))  # 300 –Ω–µ–π—Ä–æ–Ω–æ–≤  
        x = self.dropout(x)
        
        x = self.activation(self.fc3(x))  # 200 –Ω–µ–π—Ä–æ–Ω–æ–≤
        x = self.dropout(x)
        
        x = self.activation(self.fc4(x))  # 100 –Ω–µ–π—Ä–æ–Ω–æ–≤
        x = self.dropout(x)
        
        x = self.fc5(x)  # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        
        # –ó–∞–ø–∏—Å—å –∞–∫—Ç–∏–≤–∞—Ü–∏–π
        self._record_activations(x)
        
        return x
    
    def _record_activations(self, x):
        """–ó–∞–ø–∏—Å—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –Ω–µ–π—Ä–æ–Ω–æ–≤"""
        with torch.no_grad():
            # –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞–±–æ—Ç—ã —Å–µ—Ç–∏
            if hasattr(self, 'neuron_activations'):
                # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å –∞–∫—Ç–∏–≤–∞—Ü–∏–π
                activations = torch.mean(torch.abs(x), dim=0)
                if len(activations) <= 1000:
                    self.neuron_activations[:len(activations)] += activations.cpu().numpy()
                    self.total_activations += 1
    
    def get_neuron_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞–±–æ—Ç—ã –Ω–µ–π—Ä–æ–Ω–æ–≤"""
        if self.total_activations == 0:
            return {"error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ–± –∞–∫—Ç–∏–≤–∞—Ü–∏—è—Ö"}
        
        avg_activations = self.neuron_activations / self.total_activations
        
        return {
            "total_neurons": 1000,
            "active_neurons": np.sum(avg_activations > 0.1),
            "avg_activation": float(np.mean(avg_activations)),
            "max_activation": float(np.max(avg_activations)),
            "min_activation": float(np.min(avg_activations)),
            "activation_distribution": {
                "very_low": np.sum(avg_activations < 0.01),
                "low": np.sum((avg_activations >= 0.01) & (avg_activations < 0.1)),
                "medium": np.sum((avg_activations >= 0.1) & (avg_activations < 0.5)),
                "high": np.sum(avg_activations >= 0.5)
            }
        }

class TuxVisionNetwork(nn.Module):
    """
    –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ + –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ (–≤—Å–µ–≥–æ ~1000 –Ω–µ–π—Ä–æ–Ω–æ–≤)
    """
    
    def __init__(self, num_classes=10):
        super(TuxVisionNetwork, self).__init__()
        
        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)  # 16 —Ñ–∏–ª—å—Ç—Ä–æ–≤
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) # 32 —Ñ–∏–ª—å—Ç—Ä–∞
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) # 64 —Ñ–∏–ª—å—Ç—Ä–∞
        
        # Pooling —Å–ª–æ–∏
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ (–ø—Ä–∏–º–µ—Ä–Ω–æ 1000 –Ω–µ–π—Ä–æ–Ω–æ–≤)
        # –ü–æ—Å–ª–µ 3—Ö –ø—É–ª–∏–Ω–≥–æ–≤: 28x28 -> 14x14 -> 7x7 -> 3x3
        self.fc1 = nn.Linear(64 * 3 * 3, 512)  # 512 –Ω–µ–π—Ä–æ–Ω–æ–≤
        self.fc2 = nn.Linear(512, 256)         # 256 –Ω–µ–π—Ä–æ–Ω–æ–≤
        self.fc3 = nn.Linear(256, 128)         # 128 –Ω–µ–π—Ä–æ–Ω–æ–≤
        self.fc4 = nn.Linear(128, num_classes) # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        
        self.dropout = nn.Dropout(0.3)
        self.activation = nn.ReLU()
        
    def forward(self, x):
        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏
        x = self.pool(self.activation(self.conv1(x)))
        x = self.pool(self.activation(self.conv2(x))) 
        x = self.pool(self.activation(self.conv3(x)))
        
        # Flatten
        x = x.view(x.size(0), -1)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        x = self.activation(self.fc1(x))
        x = self.dropout(x)
        
        x = self.activation(self.fc2(x))
        x = self.dropout(x)
        
        x = self.activation(self.fc3(x))
        x = self.dropout(x)
        
        x = self.fc4(x)
        
        return x

class TuxAICore:
    """
    –Ø–¥—Ä–æ Tux AI - –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
    """
    
    def __init__(self, model_dir: str = "data/models"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # –û—Å–Ω–æ–≤–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å
        self.main_network = TuxNeuralNetwork()
        
        # –°–µ—Ç—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        self.vision_network = TuxVisionNetwork()
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        self.is_trained = False
        self.training_history = []
        
        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        self.image_size = (28, 28)  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è MNIST-like –¥–∞–Ω–Ω—ã—Ö
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.inference_count = 0
        self.training_cycles = 0
        
    def preprocess_image(self, image_path: str) -> torch.Tensor:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image = Image.open(image_path).convert('L')  # –í –æ—Ç—Ç–µ–Ω–∫–∏ —Å–µ—Ä–æ–≥–æ
            
            # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
            image = image.resize(self.image_size)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ numpy array
            img_array = np.array(image, dtype=np.float32)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è [0, 1]
            img_array = img_array / 255.0
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –¥–ª—è –±–∞—Ç—á–∞ –∏ –∫–∞–Ω–∞–ª–æ–≤
            img_tensor = torch.from_numpy(img_array).unsqueeze(0).unsqueeze(0)
            
            return img_tensor
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return torch.zeros(1, 1, *self.image_size)
    
    def analyze_image(self, image_path: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        try:
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
            image_tensor = self.preprocess_image(image_path)
            
            # –ê–Ω–∞–ª–∏–∑
            with torch.no_grad():
                self.vision_network.eval()
                output = self.vision_network(image_tensor)
                probabilities = F.softmax(output, dim=1)
                
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                _, predicted = torch.max(output, 1)
                confidence = probabilities[0][predicted].item()
                
            self.inference_count += 1
            
            return {
                "success": True,
                "prediction": int(predicted[0]),
                "confidence": float(confidence),
                "all_probabilities": probabilities[0].tolist(),
                "image_size": self.image_size
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def train_on_images(self, images_dir: str, epochs: int = 10):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–∞–±–æ—Ä–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
            # –î–ª—è –¥–µ–º–æ —Å–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            print("üéì –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö...")
            
            # –ò–º–∏—Ç–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
            for epoch in range(epochs):
                # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ
                dummy_images = torch.randn(32, 1, 28, 28)  # –ë–∞—Ç—á –∏–∑ 32 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                dummy_labels = torch.randint(0, 10, (32,))  # –°–ª—É—á–∞–π–Ω—ã–µ –º–µ—Ç–∫–∏
                
                # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
                self.vision_network.train()
                outputs = self.vision_network(dummy_images)
                loss = F.cross_entropy(outputs, dummy_labels)
                
                # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã backward pass –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
                
                # –ó–∞–ø–∏—Å—å –≤ –∏—Å—Ç–æ—Ä–∏—é
                self.training_history.append({
                    "epoch": epoch + 1,
                    "loss": loss.item(),
                    "type": "vision",
                    "samples": len(dummy_images)
                })
                
                print(f"üìä –≠–ø–æ—Ö–∞ {epoch+1}/{epochs}, –ü–æ—Ç–µ—Ä–∏: {loss.item():.4f}")
            
            self.is_trained = True
            self.training_cycles += 1
            
            return {
                "success": True,
                "epochs_completed": epochs,
                "final_loss": self.training_history[-1]["loss"],
                "training_cycles": self.training_cycles
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def process_text(self, text: str) -> torch.Tensor:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            words = text.lower().split()
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ embedding (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–µ–Ω —Å–ª–æ–≤–∞—Ä—å)
            embedding = np.zeros(784)  # –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ç–∏
            
            for i, word in enumerate(words[:50]):  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
                # –ü—Ä–æ—Å—Ç–æ–π —Ö—ç—à –¥–ª—è –¥–µ–º–æ
                hash_val = hash(word) % 784
                embedding[hash_val] = 1.0
            
            return torch.from_numpy(embedding.astype(np.float32)).unsqueeze(0)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            return torch.zeros(1, 784)
    
    def analyze_text(self, text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        try:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            text_tensor = self.process_text(text)
            
            # –ê–Ω–∞–ª–∏–∑
            with torch.no_grad():
                self.main_network.eval()
                output = self.main_network(text_tensor)
                
                # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤—ã—Ö–æ–¥–∞
                sentiment = torch.sigmoid(output[0][0]).item()  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —á—Ç–æ –ø–µ—Ä–≤—ã–π –≤—ã—Ö–æ–¥ - sentiment
                complexity = torch.sigmoid(output[0][1]).item() # –í—Ç–æ—Ä–æ–π –≤—ã—Ö–æ–¥ - —Å–ª–æ–∂–Ω–æ—Å—Ç—å
                
            self.inference_count += 1
            
            return {
                "success": True,
                "sentiment": float(sentiment),  # 0-1, –≥–¥–µ 1 - –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π
                "complexity": float(complexity), # 0-1, –≥–¥–µ 1 - —Å–ª–æ–∂–Ω—ã–π —Ç–µ–∫—Å—Ç
                "text_length": len(text),
                "words_count": len(text.split())
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def get_network_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö"""
        main_params = sum(p.numel() for p in self.main_network.parameters())
        vision_params = sum(p.numel() for p in self.vision_network.parameters())
        
        neuron_stats = self.main_network.get_neuron_statistics()
        
        return {
            "main_network": {
                "name": "TuxNeuralNetwork",
                "total_neurons": 1000,
                "parameters": main_params,
                "layers": 5,
                "activation_function": "ReLU"
            },
            "vision_network": {
                "name": "TuxVisionNetwork", 
                "parameters": vision_params,
                "conv_layers": 3,
                "fc_layers": 4,
                "activation_function": "ReLU"
            },
            "training": {
                "is_trained": self.is_trained,
                "training_cycles": self.training_cycles,
                "inference_count": self.inference_count,
                "history_entries": len(self.training_history)
            },
            "neuron_statistics": neuron_stats
        }
    
    def save_model(self, model_name: str = "tux_ai_core"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"""
        try:
            model_path = self.model_dir / model_name
            model_path.mkdir(exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞
            torch.save(self.main_network.state_dict(), model_path / "main_network.pth")
            torch.save(self.vision_network.state_dict(), model_path / "vision_network.pth")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "saved_at": str(np.datetime64('now')),
                "training_cycles": self.training_cycles,
                "inference_count": self.inference_count,
                "is_trained": self.is_trained,
                "training_history": self.training_history
            }
            
            with open(model_path / "metadata.json", 'w') as f:
                json.dump(metadata, f, indent=2)
            
            return {"success": True, "path": str(model_path)}
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def load_model(self, model_name: str = "tux_ai_core"):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π"""
        try:
            model_path = self.model_dir / model_name
            
            if not model_path.exists():
                return {"success": False, "error": "–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"}
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
            self.main_network.load_state_dict(torch.load(model_path / "main_network.pth"))
            self.vision_network.load_state_dict(torch.load(model_path / "vision_network.pth"))
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open(model_path / "metadata.json", 'r') as f:
                metadata = json.load(f)
                
            self.training_cycles = metadata.get("training_cycles", 0)
            self.inference_count = metadata.get("inference_count", 0)
            self.is_trained = metadata.get("is_trained", False)
            self.training_history = metadata.get("training_history", [])
            
            return {"success": True, "loaded_from": str(model_path)}
            
        except Exception as e:
            return {"success": False, "error": str(e)}

# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def demonstrate_tux_ai():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π Tux AI Core"""
    print("üêß –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Tux AI Neural Network")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —è–¥—Ä–∞
    tux_ai = TuxAICore()
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ—Ç—è—Ö
    info = tux_ai.get_network_info()
    print("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö:")
    print(f"   –û—Å–Ω–æ–≤–Ω–∞—è —Å–µ—Ç—å: {info['main_network']['total_neurons']} –Ω–µ–π—Ä–æ–Ω–æ–≤")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {info['main_network']['parameters']}")
    print(f"   –°–ª–æ–∏: {info['main_network']['layers']}")
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞
    print("\nüí¨ –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞:")
    sample_text = "Tux AI - —ç—Ç–æ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å 1000 –Ω–µ–π—Ä–æ–Ω–æ–≤!"
    result = tux_ai.analyze_text(sample_text)
    
    if result["success"]:
        print(f"   –¢–µ–∫—Å—Ç: '{sample_text}'")
        print(f"   –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {result['sentiment']:.2f}")
        print(f"   –°–ª–æ–∂–Ω–æ—Å—Ç—å: {result['complexity']:.2f}")
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    print("\nüéì –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:")
    training_result = tux_ai.train_on_images("demo", epochs=3)
    
    if training_result["success"]:
        print(f"   –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"   –¶–∏–∫–ª–æ–≤ –æ–±—É—á–µ–Ω–∏—è: {training_result['training_cycles']}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–µ–π—Ä–æ–Ω–æ–≤
    print("\nüß† –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–µ–π—Ä–æ–Ω–æ–≤:")
    stats = tux_ai.main_network.get_neuron_statistics()
    if "error" not in stats:
        print(f"   –ê–∫—Ç–∏–≤–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤: {stats['active_neurons']}/1000")
        print(f"   –°—Ä–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è: {stats['avg_activation']:.3f}")
    
    print("\nüéâ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

if __name__ == "__main__":
    demonstrate_tux_ai()